version: '3'

services:
  namenode:
    env_file: config.sh
    image: 'uhopper/hadoop-namenode'
    container_name: namenode
    hostname: namenode
    ports:
      - "8020:8020"
      - '50070:50070'
    volumes:
      - $NAME_HOME:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=$CLUSTER_NAME

  datanode:
    env_file: ./config.sh
    image: "uhopper/hadoop-datanode"
    container_name: datanode
    hostname: datanode
    volumes:
      - $DATA_HOME:/hadoop/dfs/data
    ports:
      - "111:111/tcp"
      - "111:111/udp"
      - "2049:2049"
      - "4242:4242"
      - "4242:4242/udp"
    environment:
      # - NOTHING
      - CLUSTER_NAME=$CLUSTER_NAME
      # must set as env here, or shell will not allowed
      - CORE_CONF_nfs_exports_allowed_hosts=* rw
      - HDFS_CONF_nfs_exports_allowed_hosts=* rw
    depends_on:
      - "namenode"

  resource:
    env_file: ./config.sh
    image: uhopper/hadoop-resourcemanager
    hostname: resource
    ports:
      - "8088:8088"
      - "8030:8030"
      - "8031:8031"
      - "8032:8032"
      - "8033:8033"
    environment:
      - NOTHING
    depends_on:
      - "datanode"

  nodemanager:
    env_file: ./config.sh
    image: uhopper/hadoop-nodemanager
    hostname: nodemanager    
    ports:
      - "8040:8040"
      - "8041:8041"
      - "8042:8042"
    environment:
      - NOTHING
    depends_on:
      - "resource"
  
  spark:
    image: uhopper/hadoop-spark
    domainname: hadoop
    environment:
      - NOTHING
    command: tail -f /var/log/dmesg
    depends_on:
      - "nodemanager"

networks:
  default:
    external:
      name: $NETWORK